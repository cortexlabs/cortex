# Copyright 2019 Cortex Labs, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# Spark Builtin: Approximate the number of distinct values in a column.
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.approx_count_distinct
- kind: aggregator
  name: approx_count_distinct
  path: spark/approx_count_distinct.py
  output_type: INT
  inputs:
    columns:
      col: FLOAT_COLUMN|INT_COLUMN|STRING_COLUMN
    args:
      rsd: FLOAT

# Spark Builtin: Calculate the average of the column.
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.avg
- kind: aggregator
  name: avg
  path: spark/avg.py
  output_type: FLOAT
  inputs:
    columns:
      col: FLOAT_COLUMN|INT_COLUMN

# Spark Builtin: Accumalate all of the unique int values in the int col.
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.collect_set
- kind: aggregator
  name: collect_set_int
  path: spark/collect_set_int.py
  output_type: [INT]
  inputs:
    columns:
      col: INT_COLUMN

# Spark Builtin: Accumalate all of the unique int values in the float col.
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.collect_set
- kind: aggregator
  name: collect_set_float
  path: spark/collect_set_float.py
  output_type: [FLOAT]
  inputs:
    columns:
      col: FLOAT_COLUMN

# Spark Builtin: Accumalate all of the unique int values in the string col.
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.collect_set
- kind: aggregator
  name: collect_set_string
  path: spark/collect_set_string.py
  output_type: [STRING]
  inputs:
    columns:
      col: STRING_COLUMN

# Spark Builtin: Count the number of values in the column.
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.count
- kind: aggregator
  name: count
  path: spark/count.py
  output_type: INT
  inputs:
    columns:
      col: INT_COLUMN|FLOAT_COLUMN|STRING_COLUMN

# Spark Builtin: Given a group of columns, count the unique rows in the group columns.
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.countDistinct
- kind: aggregator
  name: count_distinct
  path: spark/count_distinct.py
  output_type: INT
  inputs:
    columns:
      col: INT_COLUMN|FLOAT_COLUMN|STRING_COLUMN
      cols: [INT_COLUMN|FLOAT_COLUMN|STRING_COLUMN]

# Spark Builtin: Calculate the population covariance between col1 and col2 (scaled by 1/N).
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.covar_pop
- kind: aggregator
  name: covar_pop
  path: spark/covar_pop.py
  output_type: FLOAT
  inputs:
    columns:
      col1: INT_COLUMN|FLOAT_COLUMN
      col2: INT_COLUMN|FLOAT_COLUMN


# Spark Builtin: Calculate the sample covariance between col1 and col2 (scaled by 1/(N-1)).
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.covar_samp
- kind: aggregator
  name: covar_samp
  path: spark/covar_samp.py
  output_type: FLOAT
  inputs:
    columns:
      col1: INT_COLUMN|FLOAT_COLUMN
      col2: INT_COLUMN|FLOAT_COLUMN

# Spark Builtin: Calculate the sharpness of the peak of a frequency-distribution of the input column.
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.kurtosis
- kind: aggregator
  name: kurtosis
  path: spark/kurtosis.py
  output_type: FLOAT
  inputs:
    columns:
      col: INT_COLUMN|FLOAT_COLUMN

# Spark Builtin: Get the max value of the input int column.
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.max
- kind: aggregator
  name: max_int
  path: spark/max_int.py
  output_type: INT
  inputs:
    columns:
      col: INT_COLUMN

# Spark Builtin: Get the max value of the input float column.
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.max
- kind: aggregator
  name: max_float
  path: spark/max_float.py
  output_type: FLOAT
  inputs:
    columns:
      col: FLOAT_COLUMN

# Spark Builtin: Get the max value of the input string column.
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.max
- kind: aggregator
  name: max_string
  path: spark/max_string.py
  output_type: STRING
  inputs:
    columns:
      col: STRING_COLUMN

# Spark Builtin: Calculate the mean of the values in the input column.
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.mean
- kind: aggregator
  name: mean
  path: spark/mean.py
  output_type: FLOAT
  inputs:
    columns:
      col: FLOAT_COLUMN|INT_COLUMN

# Spark Builtin: Get the min value of the intput int column.
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.min
- kind: aggregator
  name: min_int
  path: spark/min_int.py
  output_type: INT
  inputs:
    columns:
      col: INT_COLUMN

# Spark Builtin: Get the min value of the input float column.
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.min
- kind: aggregator
  name: min_float
  path: spark/min_float.py
  output_type: FLOAT
  inputs:
    columns:
      col: FLOAT_COLUMN

# Spark Builtin: Get the min value of the input string column.
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.min
- kind: aggregator
  name: min_string
  path: spark/min_string.py
  output_type: STRING
  inputs:
    columns:
      col: STRING_COLUMN

# Spark Builtin: Calculate the skewness of the values in the input column.
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.skewness
- kind: aggregator
  name: skewness
  path: spark/skewness.py
  output_type: FLOAT
  inputs:
    columns:
      col: FLOAT_COLUMN|INT_COLUMN

# Spark Builtin: Calculate the standard deviation (scaled by 1/(N-1)).
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.stddev
- kind: aggregator
  name: stddev
  path: spark/stddev.py
  output_type: FLOAT
  inputs:
    columns:
      col: FLOAT_COLUMN|INT_COLUMN

# Spark Builtin: Calculate the standard deviation (scaled by 1/(N)).
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.stddev_pop
- kind: aggregator
  name: stddev_pop
  path: spark/stddev_pop.py
  output_type: FLOAT
  inputs:
    columns:
      col: FLOAT_COLUMN|INT_COLUMN

# Spark Builtin: Calculate the standard deviation (scaled by 1/(N-1)).
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.stddev_samp
- kind: aggregator
  name: stddev_samp
  path: spark/stddev_samp.py
  output_type: FLOAT
  inputs:
    columns:
      col: FLOAT_COLUMN|INT_COLUMN

# Spark Builtin: Sum all of the values in the input int column.
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.sum
- kind: aggregator
  name: sum_int
  path: spark/sum_int.py
  output_type: INT
  inputs:
    columns:
      col: INT_COLUMN

# Spark Builtin: Sum all of the values in the input float column.
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.sum
- kind: aggregator
  name: sum_float
  path: spark/sum_float.py
  output_type: FLOAT
  inputs:
    columns:
      col: FLOAT_COLUMN

# Spark Builtin: Sum all of the distinct values in the input int column.
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.sumDistinct
- kind: aggregator
  name: sum_distinct_int
  path: spark/sum_distinct_int.py
  output_type: INT
  inputs:
    columns:
      col: INT_COLUMN


# Spark Builtin: Sum all of the distinct values in the input float column.
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.sumDistinct
- kind: aggregator
  name: sum_distinct_float
  path: spark/sum_distinct_float.py
  output_type: FLOAT
  inputs:
    columns:
      col: FLOAT_COLUMN

# Spark Builtin: Calculate the variance (scaled by 1/(N)).
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.var_pop
- kind: aggregator
  name: var_pop
  path: spark/var_pop.py
  output_type: FLOAT
  inputs:
    columns:
      col: FLOAT_COLUMN|INT_COLUMN

# Spark Builtin: Calculate the variance (scaled by 1/(N-1)).
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.var_samp
- kind: aggregator
  name: var_samp
  path: spark/var_samp.py
  output_type: FLOAT
  inputs:
    columns:
      col: FLOAT_COLUMN|INT_COLUMN

# Spark Builtin: Calculate the variance (scaled by 1/(N-1)).
# source: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.variance
- kind: aggregator
  name: variance
  path: spark/variance.py
  output_type: FLOAT
  inputs:
    columns:
      col: FLOAT_COLUMN|INT_COLUMN

# Given the number of buckets, calculates the boundaries of a bucket such that the values in the column can be evenly
# split into the buckets. Works well with transformers.bucketize.
# For example: An input column with the following values [1,2,3,4,5,6] and num_buckets=3
# would return [-inf, 2.0, 4.0, inf],
- kind: aggregator
  name: bucket_boundaries
  path: bucket_boundaries.py
  output_type: [FLOAT]
  inputs:
    columns:
      col: FLOAT_COLUMN|INT_COLUMN
    args:
      num_buckets: INT

# Enumerates the unique values in a string column and orders them by placing the unique strings in
# list ordered by most frequent starting at the 0th index.
# Works well in conjunction with transformers.index_string.
# For example: An input column with the following values ['t', 'f', 't'] would return
# {"index": ['t', 'f'], "reversed_index": {'t': 0, 'f': 1}}.
- kind: aggregator
  name: index_string
  path: index_string.py
  output_type: {"index": [STRING], "reversed_index": {STRING: INT}}
  inputs:
    columns:
      col: STRING_COLUMN

# Counts the occurrences of each value in the string input column and divides the counts by the total number of values.
# For example: An input column with the following values ['t', 'f', 't', 't'] would return {'t': 0.75, 'f': 0.25}.
- kind: aggregator
  name: class_distribution_string
  path: class_distribution.py
  output_type: {STRING: FLOAT}
  inputs:
    columns:
      col: STRING_COLUMN

# Counts the occurrences of each value in the int input column and divides the counts by the total number of values.
# For example: An input column with the following values [1, 2, 3, 1] would return {1: 0.5, 2: 0.25, 3: 0.25}.
- kind: aggregator
  name: class_distribution_int
  path: class_distribution.py
  output_type: {INT: FLOAT}
  inputs:
    columns:
      col: INT_COLUMN
