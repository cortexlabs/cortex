{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iris_sklearn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiTxCwB7t6Ef",
        "colab_type": "text"
      },
      "source": [
        "# Training an Iris classifier using scikit-learn\n",
        "In this notebook, we'll show how to train a classifier trained on the [iris data set](https://archive.ics.uci.edu/ml/datasets/iris) using scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6QdLAUpuW7r",
        "colab_type": "text"
      },
      "source": [
        "## Install Dependencies\n",
        "First, we'll install our dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQE5z_kHj9jV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install scikit-learn==0.21.* onnxmltools==1.5.* boto3==1.*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEVK-sLnumqn",
        "colab_type": "text"
      },
      "source": [
        "## Load the data\n",
        "We can use scikit-learn to load the Iris dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx9Xw0x0lfbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obGdgMm3urb2",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge1aH9ScmYrk",
        "colab_type": "text"
      },
      "source": [
        "We would like to normalize the data before training the model. We can use sklearn's [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjYp8TaflhW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMWrB8UwnlDR",
        "colab_type": "text"
      },
      "source": [
        "Now we can split the dataset for training/evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Dim142knm18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3022fHZ_no9Z",
        "colab_type": "text"
      },
      "source": [
        "We'll use scikit-learn's [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTe6POonnqcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg_model = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\")\n",
        "logreg_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Test data accuracy: {:.2f}\".format(logreg_model.score(X_test, y_test)))  # Accuracy should be > 90%"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdwu-wzJvJLb",
        "colab_type": "text"
      },
      "source": [
        "## Export the model\n",
        "Now we can export the model in the ONNX format:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVgs2mkdllRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from onnxmltools import convert_sklearn\n",
        "from onnxconverter_common.data_types import FloatTensorType\n",
        "\n",
        "onnx_model = convert_sklearn(logreg_model, initial_types=[(\"input\", FloatTensorType([1, 4]))])\n",
        "\n",
        "with open(\"sklearn.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipVlP4yPxFxw",
        "colab_type": "text"
      },
      "source": [
        "## Upload the model to AWS\n",
        "\n",
        "Cortex loads models from AWS, so we need to upload the exported model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IqsfyylxLhy",
        "colab_type": "text"
      },
      "source": [
        "Set these variables to configure your AWS credentials and model upload path:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc9LBH1uHT_h",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "AWS_ACCESS_KEY_ID = \"\" #@param {type:\"string\"}\n",
        "AWS_SECRET_ACCESS_KEY = \"\" #@param {type:\"string\"}\n",
        "S3_UPLOAD_PATH = \"s3://my-bucket/iris/sklearn.onnx\" #@param {type:\"string\"}\n",
        "\n",
        "import sys\n",
        "import re\n",
        "\n",
        "if AWS_ACCESS_KEY_ID == \"\":\n",
        "    print(\"\\033[91m{}\\033[00m\".format(\"ERROR: Please set AWS_ACCESS_KEY_ID\"), file=sys.stderr)\n",
        "\n",
        "elif AWS_SECRET_ACCESS_KEY == \"\":\n",
        "    print(\"\\033[91m{}\\033[00m\".format(\"ERROR: Please set AWS_SECRET_ACCESS_KEY\"), file=sys.stderr)\n",
        "\n",
        "else:\n",
        "    try:\n",
        "        bucket = re.search(\"s3://(.+?)/\", S3_UPLOAD_PATH).group(1)\n",
        "        key = re.search(\"s3://.+?/(.+)\", S3_UPLOAD_PATH).group(1)\n",
        "    except:\n",
        "        print(\"\\033[91m{}\\033[00m\".format(\"ERROR: Invalid s3 path (should be of the form s3://my-bucket/path/to/file)\"), file=sys.stderr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXeuZsaQxUc8",
        "colab_type": "text"
      },
      "source": [
        "Upload the model to S3:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLmnWTEVsu55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import boto3\n",
        "\n",
        "s3 = boto3.client(\"s3\", aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
        "print(\"Uploading {} ...\".format(S3_UPLOAD_PATH), end = '')\n",
        "s3.upload_file(\"sklearn.onnx\", bucket, key)\n",
        "print(\" ✓\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndOZznTArYSf",
        "colab_type": "text"
      },
      "source": [
        "<!-- CORTEX_VERSION_MINOR -->\n",
        "We also need to upload the mean and standard deviation, so that the [pre-inference request handler](https://github.com/cortexlabs/cortex/blob/master/examples/iris-classifier/handlers/sklearn.py) can normalize the data before making real-time predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnwL9Mnwsmih",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "METADATA_S3_UPLOAD_PATH = \"s3://my-bucket/iris/scalars.json\" #@param {type:\"string\"}\n",
        "\n",
        "try:\n",
        "    metadata_bucket = re.search('s3://(.+?)/', METADATA_S3_UPLOAD_PATH).group(1)\n",
        "    metadata_key = re.search('s3://.+?/(.+)', METADATA_S3_UPLOAD_PATH).group(1)\n",
        "except:\n",
        "    print(\"\\033[91m{}\\033[00m\".format(\"ERROR: Invalid s3 path (should be of the form s3://my-bucket/path/to/file)\"), file=sys.stderr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp9ZILNttj-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import json\n",
        "\n",
        "metadata = {\n",
        "    \"mean\": scaler.mean_.tolist(),\n",
        "    \"stddev\": [math.sqrt(x) for x in scaler.var_],\n",
        "}\n",
        "\n",
        "print(\"Uploading {} ...\".format(METADATA_S3_UPLOAD_PATH), end = '')\n",
        "s3.put_object(Body=json.dumps(metadata), Bucket=metadata_bucket, Key=metadata_key)\n",
        "print(\" ✓\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR-mmcUzyCV3",
        "colab_type": "text"
      },
      "source": [
        "<!-- CORTEX_VERSION_MINOR -->\n",
        "That's it! See the [example on GitHub](https://github.com/cortexlabs/cortex/tree/master/examples/iris-classifier) for how to deploy the model as an API."
      ]
    }
  ]
}
