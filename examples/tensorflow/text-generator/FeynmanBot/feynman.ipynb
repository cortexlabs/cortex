{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How To Build A Feynman Bot",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc5cIgeEmv8o",
        "colab_type": "text"
      },
      "source": [
        "# Creating a Richard Feynman bot with GPT-2\n",
        "In this notebook, we're going to retrain [OpenAI's GPT-2 text generation model](https://github.com/openai/gpt-2), and then export that trained model. You can use any text you'd likeâ€”we chose Richard Feynman's lectures because we're fans, and the lectures are [freely available.](http://www.feynmanlectures.caltech.edu/I_toc.html)\n",
        "\n",
        "At the end of this tutorial, we're going to deploy our trained model as a web API on AWS using Cortex.\n",
        "\n",
        "Cortex is 100% free and open source. All we ask is that if you like this tutorial, you click here to leave a star on the [Cortex repository](https://github.com/cortexlabs/cortex/).\n",
        "\n",
        "Also, as a quick thank you note, this tutorial uses Max Woolf's incredible [gpt-2-simple](https://github.com/minimaxir/gpt-2-simple) to train GPT-2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hprb4qSqF2O",
        "colab_type": "text"
      },
      "source": [
        "# Initial Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz2ioOcpoPjV",
        "colab_type": "text"
      },
      "source": [
        "To begin, we'll install the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk4Q2RR-UZQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==1.14.* numpy==1.* boto3==1.* gpt-2-simple"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkVf5FmuUMrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.saved_model.signature_def_utils_impl import predict_signature_def\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ay7qiQFoWRn",
        "colab_type": "text"
      },
      "source": [
        "Thanks to gpt-2-simple, we can easily download the GPT-2 model with one command. While there are several sizes of GPT-2, we're going to be using the smallest here for simplicity and speed's sake."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk9Mzopjq-7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgltOotirPmA",
        "colab_type": "text"
      },
      "source": [
        "Next, we are going to mount our Google Drive to Colab, so that we can easily upload our text file, and export our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9taM0lmrZ-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3xyg2v7sT8c",
        "colab_type": "text"
      },
      "source": [
        "Finally, we will upload our text file. You can upload a file by clicking into the expanded pane on the left side of your notebook in Colab, selecting the *Files* tab, and uploading. Once your file is uploaded, input its name into the following cell and click run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU6HBaaktF5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = \"feynmann.txt\"\n",
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKUg2qoPtbbk",
        "colab_type": "text"
      },
      "source": [
        "# Training GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF6qwKRctlKa",
        "colab_type": "text"
      },
      "source": [
        "Training GPT-2 is incredibly easy thanks to gpt-2-simple. We're not going to worry about the parameters here, but if you want to dig deeper, check out the documentation. Simply run the following code to finetune your model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOgT-s9pucCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='124M',\n",
        "              steps=200,\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=100,\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMf9lQS8wBmp",
        "colab_type": "text"
      },
      "source": [
        "Once your model is trained, you can save it to your Google Drive with the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI8iSrb8wHol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWKSRgLuwwgK",
        "colab_type": "text"
      },
      "source": [
        "Done! Now you can export your model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT1O0ByOw0-g",
        "colab_type": "text"
      },
      "source": [
        "# Exporting your GPT-2 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1E4xxltw6dc",
        "colab_type": "text"
      },
      "source": [
        "The following code will export your model for TensorFlow Serving, which will allow Cortex to deploy it as a web API on AWS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdnYXr1IKaF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gpt_2_simple.src import model, sample, encoder, memory_saving_gradients\n",
        "\n",
        "def export_for_serving(\n",
        "    model_name='run1',\n",
        "    seed=None,\n",
        "    batch_size=1,\n",
        "    length=None,\n",
        "    temperature=1,\n",
        "    top_k=0,\n",
        "    models_dir='/content/drive/My Drive/checkpoint'\n",
        "):\n",
        "    \"\"\"\n",
        "    Export the model for TF Serving\n",
        "    :model_name=124M : String, which model to use\n",
        "    :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
        "     results\n",
        "    :length=None : Number of tokens in generated text, if None (default), is\n",
        "     determined by model hyperparameters\n",
        "    :temperature=1 : Float value controlling randomness in boltzmann\n",
        "     distribution. Lower temperature results in less random completions. As the\n",
        "     temperature approaches zero, the model will become deterministic and\n",
        "     repetitive. Higher temperature results in more random completions.\n",
        "    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
        "     considered for each step (token), resulting in deterministic completions,\n",
        "     while 40 means 40 words are considered at each step. 0 (default) is a\n",
        "     special setting meaning no restrictions. 40 generally is a good value.\n",
        "     :models_dir : path to parent folder containing model subfolders\n",
        "     (i.e. contains the <model_name> folder)\n",
        "    \"\"\"\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if length is None:\n",
        "        length = hparams.n_ctx\n",
        "    elif length > hparams.n_ctx:\n",
        "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    with tf.Session(graph=tf.Graph()) as sess:\n",
        "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        np.random.seed(seed)\n",
        "        tf.set_random_seed(seed)\n",
        "\n",
        "        output = sample.sample_sequence(\n",
        "            hparams=hparams, length=length,\n",
        "            context=context,\n",
        "            batch_size=batch_size,\n",
        "            temperature=temperature, top_k=top_k\n",
        "        )\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "\n",
        "        export_dir=os.path.join(models_dir, model_name, \"export\", str(time.time()).split('.')[0])\n",
        "        if not os.path.isdir(export_dir):\n",
        "            os.makedirs(export_dir)\n",
        "\n",
        "        builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n",
        "        signature = predict_signature_def(inputs={'context': context},\n",
        "        outputs={'sample': output})\n",
        "\n",
        "        builder.add_meta_graph_and_variables(sess,\n",
        "                                     [tf.saved_model.SERVING],\n",
        "                                     signature_def_map={\"predict\": signature},\n",
        "                                     strip_default_attrs=True)\n",
        "        builder.save()\n",
        "\n",
        "\n",
        "export_for_serving(top_k=40, length=256, model_name='run1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGfSohMrowmg",
        "colab_type": "text"
      },
      "source": [
        "## Upload the model to AWS\n",
        "\n",
        "Cortex loads models from AWS, so we need to upload the exported model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfB5QZ82ozj9",
        "colab_type": "text"
      },
      "source": [
        "Set these variables to configure your AWS credentials and model upload path:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2RNuNk7o1c5",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "AWS_ACCESS_KEY_ID = \"\" #@param {type:\"string\"}\n",
        "AWS_SECRET_ACCESS_KEY = \"\" #@param {type:\"string\"}\n",
        "S3_UPLOAD_PATH = \"\" #@param {type:\"string\"}\n",
        "\n",
        "import sys\n",
        "import re\n",
        "\n",
        "if AWS_ACCESS_KEY_ID == \"\":\n",
        "    print(\"\\033[91m {}\\033[00m\".format(\"ERROR: Please set AWS_ACCESS_KEY_ID\"), file=sys.stderr)\n",
        "\n",
        "elif AWS_SECRET_ACCESS_KEY == \"\":\n",
        "    print(\"\\033[91m {}\\033[00m\".format(\"ERROR: Please set AWS_SECRET_ACCESS_KEY\"), file=sys.stderr)\n",
        "\n",
        "else:\n",
        "    try:\n",
        "        bucket = re.search(\"s3://(.+?)/\", S3_UPLOAD_PATH).group(1)\n",
        "        key = re.search(\"s3://.+?/(.+)\", S3_UPLOAD_PATH).group(1)\n",
        "    except:\n",
        "        print(\"\\033[91m {}\\033[00m\".format(\"ERROR: Invalid s3 path (should be of the form s3://my-bucket/path/to/file)\"), file=sys.stderr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ics0omsrpS8V",
        "colab_type": "text"
      },
      "source": [
        "Upload the model to S3:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnKncToppUhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import boto3\n",
        "\n",
        "s3 = boto3.client(\"s3\", aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
        "\n",
        "MODEL_NAME = 'gpt-2'\n",
        "\n",
        "for dirpath, _, filenames in os.walk(\"content/drive/My Drive/checkpoint/run1/export\"):\n",
        "    for filename in filenames:\n",
        "        filepath = os.path.join(dirpath, filename)\n",
        "        filekey = os.path.join(key, MODEL_NAME, filename)\n",
        "        print(\"Uploading s3://{}/{} ...\".format(bucket, filekey), end = '')\n",
        "        s3.upload_file(filepath, bucket, filekey)\n",
        "        print(\" âœ“\")\n",
        "\n",
        "print(\"\\nUploaded model export directory to {}/{}\".format(S3_UPLOAD_PATH, MODEL_NAME))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIMVPhe2qkU4",
        "colab_type": "text"
      },
      "source": [
        "<!-- CORTEX_VERSION_MINOR x2 -->\n",
        "We also need to upload `vocab.bpe` and `encoder.json`, so that the [encoder](https://github.com/cortexlabs/cortex/blob/master/examples/text-generator/encoder.py) in the [pre-inference request handler](https://github.com/cortexlabs/cortex/blob/master/examples/text-generator/handler.py) can encode the input text before making a request to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdN8MtZxsO9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Uploading s3://{}/{}/vocab.bpe ...\".format(bucket, key), end = '')\n",
        "s3.upload_file(\"/content/drive/My Drive/checkpoint/run1/vocab.bpe\", bucket, os.path.join(key, \"vocab.bpe\"))\n",
        "print(\" âœ“\")\n",
        "\n",
        "print(\"Uploading s3://{}/{}/encoder.json ...\".format(bucket, key), end = '')\n",
        "s3.upload_file(\"/content/drive/My Drive/checkpoint/run1/encoder.json\", bucket, os.path.join(key, \"encoder.json\"))\n",
        "print(\" âœ“\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsoxwahIpnTO",
        "colab_type": "text"
      },
      "source": [
        "And that's it! Your model is uploaded to S3, and your ready to spin up a Cortex deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwSAK_NYdhMJ",
        "colab_type": "text"
      },
      "source": [
        "# Deploying with Cortex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB-90F_6drIB",
        "colab_type": "text"
      },
      "source": [
        "Deploying a model with Cortex is fairly straightforward. To deploy GPT-2, we need to do the following:\n",
        "\n",
        "1.  Install Cortex, if you haven't already.\n",
        "2.  Create a handler script. This will take data sent to our API, wrangle it to be processable by our model, and then format the model's output before returning it to the user.\n",
        "3. Configure our Cortex deployment.\n",
        "\n",
        "Installing Cortex is a quick process, which we have a [guide to here.](https://www.cortex.dev/install)\n",
        "\n",
        "Once Cortex is installed, we need to create our handler script. For our purposes, this handler script will be very simple. The code below is not executable, but you can copy + paste it into your own `handler.py` file, or download it from [this gist.](https://gist.github.com/caleb-kaiser/e8d954987abdd96ca6db163c38f302f2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-5ApsuFjBV9",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "from encoder import get_encoder\n",
        "encoder = get_encoder()\n",
        "\n",
        "\n",
        "def pre_inference(sample, metadata):\n",
        "    context = encoder.encode(sample[\"text\"])\n",
        "    return {\"context\": [context]}\n",
        "\n",
        "\n",
        "def post_inference(prediction, metadata):\n",
        "    response = prediction[\"sample\"]\n",
        "    return encoder.decode(response)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j70js8CCjjel",
        "colab_type": "text"
      },
      "source": [
        "With our handler script set up, we can now configure our Cortex deployment. A Cortex deployment uses a configuration file titled `cortex.yaml` and uses it as a blueprint for spinning up your API on AWS. For this API, our `cortex.yaml` will look like this (as before, you can download this file directly from [this gist](https://gist.github.com/caleb-kaiser/39ba06b8b9ff642c37e29d302ecb34cf).):\n",
        "\n",
        "```\n",
        "- kind: deployment\n",
        "  name: text\n",
        "\n",
        "- kind: api\n",
        "  name: generator\n",
        "  model: s3://your/bucket/model\n",
        "  request_handler: handler.py\n",
        "```\n",
        "\n",
        "Once your `cortex.yaml` file is created, you launch your deployment. To do so, simply run `$ cortex deploy` from your command line.\n",
        "\n",
        "Once this command is run, Cortex containerizes the model, makes it servable using TensorFlow Serving, exposes the endpoint with a load balancer, and orchestrates the workload on Kubernetes.\n",
        "\n",
        "You can track the status of a deployment using cortex get:\n",
        "\n",
        "```$ cortex get generator --watch\n",
        "\n",
        "status   up-to-date   available   requested   last update   avg latency\n",
        "live     1            1           1           8s            â€”\n",
        "```\n",
        "\n",
        "The output above indicates that one replica of the API was requested and one replica is available to serve predictions. Cortex will automatically launch more replicas if the load increases and spin down replicas if there is unused capacity.\n",
        "\n",
        "Congraulations! Your GPT-2 model is trained and deployed as an API.\n",
        "\n",
        "# Testing your GPT-2 API\n",
        "\n",
        "Now that your API is live, you can test it by running these commands:\n",
        "```\n",
        "$ cortex get generator\n",
        "\n",
        "url: http://***.amazonaws.com/text/generator\n",
        "\n",
        "$ curl http://***.amazonaws.com/text/generator \\\n",
        "    -X POST -H \"Content-Type: application/json\" \\\n",
        "    -d '{\"text\": \"machine learning\"}'\n",
        "\n",
        "Machine learning, with more than one thousand researchers around the world today, are looking to create computer-driven machine learning algorithms that can also be applied to human and social problems, such as education, health care, employment, medicine, politics, or the environment...\n",
        "```\n",
        "\n",
        "That's it. You now have a functional API that can receive text, and run it through GPT-2, and return a response.\n",
        "\n",
        "If you have any questions, feel free to [reach out to us directly](https://gitter.im/cortexlabs/cortex). If you enjoyed this tutorial at all, don't forget to leave a star on the [Cortex repository](https://github.com/cortexlabs/cortex).\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}